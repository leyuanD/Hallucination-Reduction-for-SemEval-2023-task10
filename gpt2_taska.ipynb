{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for Data Preparation\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Library for Classificaton Model\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2ForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "! git clone https://github.com/rewire-online/edos.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV files\n",
    "csv_labelled_aggregated = 'edos/data/edos_labelled_aggregated.csv'\n",
    "csv_labelled_individual_annotations = 'edos/data/edos_labelled_individual_annotations.csv'\n",
    "csv_gab_unlabelled = 'edos/data/gab_1M_unlabelled.csv'\n",
    "csv_reddit_unlabelled = 'edos/data/reddit_1M_unlabelled.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labelled data\n",
    "data_labelled_1 = pd.read_csv(csv_labelled_aggregated)\n",
    "data_labelled_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labelled data\n",
    "data_labelled_2 = pd.read_csv(csv_labelled_individual_annotations)\n",
    "data_labelled_2 = data_labelled_2.drop(columns=['annotator'], inplace=False)\n",
    "data_labelled_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat data_labelled_1 and data_labelled_2\n",
    "data_labelled = pd.concat([data_labelled_1, data_labelled_2])\n",
    "data_labelled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Task A\n",
    "selected_columns_taskA = ['text', 'label_sexist', 'split']\n",
    "data_labelled_taskA = data_labelled[selected_columns_taskA]\n",
    "data_labelled_taskA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train, Validate and Test\n",
    "data_train_taskA = data_labelled_taskA[data_labelled_taskA['split'] == 'train']\n",
    "data_train_taskA = data_train_taskA.drop('split', axis=1)\n",
    "print(\"Train Data:\\n\", data_train_taskA.count(), \"\\n\")\n",
    "\n",
    "data_val_taskA = data_labelled_taskA[data_labelled_taskA['split'] == 'dev']\n",
    "data_val_taskA = data_val_taskA.drop('split', axis=1)\n",
    "print(\"Validation Data:\\n\", data_val_taskA.count(), \"\\n\")\n",
    "\n",
    "data_test_taskA = data_labelled_taskA[data_labelled_taskA['split'] == 'test']\n",
    "data_test_taskA = data_test_taskA.drop('split', axis=1)\n",
    "print(\"Test Data:\\n\", data_test_taskA.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
    "    text = re.sub('\\t', ' ',  text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    text=re.sub(r'[^\\w\\s]','',text)\n",
    "    text=text.replace(\"[URL]\",\"\")\n",
    "    text=text.replace(\"[USER]\",\"\")\n",
    "    text=re.sub(r\"[â€™ºðŸ‡˜Žµ±¤£‘Œ”œ]\",\"\",text)\n",
    "    return text\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "# to make all texts lowercase\n",
    "def to_lowercase(input_text):\n",
    "\n",
    "    return input_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_taskA['text'] = data_train_taskA['text'].apply(clean_text)\n",
    "data_train_taskA['text'] = data_train_taskA['text'].apply(denoise_text)\n",
    "data_train_taskA['text'] = data_train_taskA['text'].apply(to_lowercase)\n",
    "\n",
    "data_val_taskA['text'] = data_val_taskA['text'].apply(clean_text)\n",
    "data_val_taskA['text'] = data_val_taskA['text'].apply(denoise_text)\n",
    "data_val_taskA['text'] = data_val_taskA['text'].apply(to_lowercase)\n",
    "\n",
    "data_test_taskA['text'] = data_test_taskA['text'].apply(clean_text)\n",
    "data_test_taskA['text'] = data_test_taskA['text'].apply(denoise_text)\n",
    "data_test_taskA['text'] = data_test_taskA['text'].apply(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeximsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        record = self.data.iloc[index]\n",
    "        text = record['text']\n",
    "        label = record['label_sexist']\n",
    "        if label == 'not sexist':\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        return {'text': text, 'label': label} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SeximsDataset(data_train_taskA)\n",
    "val_dataset = SeximsDataset(data_val_taskA)\n",
    "test_dataset = SeximsDataset(data_test_taskA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GPT-2 for the taska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ClassificationCollator(object):\n",
    "    def __init__(self, tokenizer, max_seq_len=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        return\n",
    "    \n",
    "    def __call__(self, sequences):\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        labels = [int(sequence['label']) for sequence in sequences]\n",
    "        inputs = self.tokenizer(\n",
    "            text=texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_seq_len,\n",
    "        )\n",
    "        inputs.update({'labels': torch.tensor(labels)})\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_classification_collator = Gpt2ClassificationCollator(tokenizer, max_seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=gpt2_classification_collator,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=gpt2_classification_collator,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=gpt2_classification_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(36)\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\", num_labels=2)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_EPOCHS = 5\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01,\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "    }\n",
    "]\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=1e-5,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "num_training_steps = TOTAL_EPOCHS * len(train_dataloader)\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer, scheduler, device_):\n",
    "    global model\n",
    "    model.train()\n",
    "\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_loss = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k: v.type(torch.long).to(device_) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss, logits = outputs[:2]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "    return true_labels, predictions_labels, total_loss\n",
    "\n",
    "def validation(dataloader, device_):\n",
    "    global model\n",
    "    model.eval()\n",
    "\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_loss = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k: v.type(torch.long).to(device_) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss, logits = outputs[:2]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "    return true_labels, predictions_labels, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "all_loss = {'train_loss': [], 'val_loss': []}\n",
    "all_acc = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n",
    "\n",
    "    train_acc = accuracy_score(y, y_pred)\n",
    "\n",
    "    y, y_pred, val_loss = validation(val_dataloader, device)\n",
    "    val_acc = accuracy_score(y, y_pred)\n",
    "\n",
    "    all_loss['train_loss'] += train_loss\n",
    "    all_loss['val_loss'] += val_loss\n",
    "\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    all_acc['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}')\n",
    "\n",
    "    model.save_pretrained('./')\n",
    "    tokenizer.save_pretrained('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "a = fig.add_subplot(4, 1, 1)\n",
    "b = fig.add_subplot(4, 1, 2)\n",
    "c = fig.add_subplot(4, 1, 3)\n",
    "\n",
    "a.plot(all_loss['train_loss'], label='Train Loss')\n",
    "b.plot(all_loss['val_loss'], label='Val Loss')\n",
    "c.plot(all_acc['train_acc'], label='Train Accuracy')\n",
    "c.plot(all_acc['val_acc'], label='Val Accuracy')\n",
    "c.set(xlabel='Epochs', ylabel='Accuracy')\n",
    "c.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y, test_y_pred, test_val_loss =validation(test_dataloader, device)\n",
    "test_val_acc = accuracy_score(test_y, test_y_pred)\n",
    "print(f'Test Loss: {torch.tensor(test_val_loss).mean():.3f}, Test Accuracy: {test_val_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
