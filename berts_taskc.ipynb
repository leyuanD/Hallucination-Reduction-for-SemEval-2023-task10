{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "%store -r data_train_taskc data_test_taskc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_taskc['label_vector'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the label distribution for taskc\n",
    "import matplotlib.pyplot as plt\n",
    "# Data\n",
    "categories = [\n",
    "    \n",
    "    \"2.1 descriptive attacks\",\n",
    "    \"2.2 aggressive and emotive attacks\",\n",
    "    \"3.1 casual use of gendered slurs, profanities, and insults\",\n",
    "    \"3.2 immutable gender differences and gender stereotypes\",\n",
    "    \"2.3 dehumanising attacks & overt sexual objectification\",\n",
    "    \"4.2 supporting systemic discrimination against women as a group\",\n",
    "    \"1.2 incitement and encouragement of harm\",\n",
    "    \"4.1 supporting mistreatment of individual women\",\n",
    "    \"3.4 condescending explanations or unwelcome advice\",\n",
    "    \"3.3 backhanded gendered compliments\",\n",
    "    \"1.1 threats of harm\"\n",
    "]\n",
    "\n",
    "counts = [\n",
    "     3281, 2581, 2472, 1472, 1042, 1026, 924, 437, 349, 285, 258\n",
    "]\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(3, 2.4))\n",
    "plt.barh(categories, counts, color='skyblue')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Category Counts')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the highest count at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to training and test set\n",
    "x = data_train_taskc.text.tolist()\n",
    "y = data_train_taskc.label_vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding data for task C\n",
    "encoding = {\n",
    "    '1.1 threats of harm': 0, \n",
    "    '1.2 incitement and encouragement of harm': 1,\n",
    "    '2.1 descriptive attacks': 2, \n",
    "    '2.2 aggressive and emotive attacks': 3,\n",
    "    '2.3 dehumanising attacks & overt sexual objectification' : 4, \n",
    "    '3.1 casual use of gendered slurs, profanities, and insults': 5,\n",
    "    '3.2 immutable gender differences and gender stereotypes':6,\n",
    "    '3.3 backhanded gendered compliments': 7, \n",
    "    '3.4 condescending explanations or unwelcome advice':8,\n",
    "    '4.1 supporting mistreatment of individual women':9,\n",
    "    '4.2 supporting systemic discrimination against women as a group': 10\n",
    "}\n",
    "\n",
    "# Integer values for each class\n",
    "y_train = [encoding[x] for x in y_train]\n",
    "y_test = [encoding[x] for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations\n",
    "def focal_loss(gamma=2., alpha=4., from_logits=False):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax if from_logits is False.\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        if from_logits:\n",
    "            y_pred = activations.softmax(y_pred)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 sub-categories classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = 'google/electra-base-discriminator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = text.Transformer(model_1, maxlen=500, class_names=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "trn_1 = t_1.preprocess_train(x_train, y_train)\n",
    "val_1 = t_1.preprocess_test(x_test, y_test)\n",
    "\n",
    "model_electra = t_1.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_electra.compile(loss=focal_loss(alpha=1, from_logits=True),optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_electra = ktrain.get_learner(model_electra, train_data=trn_1, val_data=val_1, batch_size =BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_electra.fit_onecycle(2e-5, 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = 'roberta-base' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_roberta = text.Transformer(model_2, maxlen=500, class_names=[0, 1,2,3,4,5,6,7,8,9,10])\n",
    "trn_roberta = t_roberta.preprocess_train(x_train, y_train)\n",
    "val_roberta = t_roberta.preprocess_test(x_test, y_test)\n",
    "\n",
    "model_roberta = t_roberta.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_roberta.compile(loss=focal_loss(alpha=1, from_logits=True),optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_roberta = ktrain.get_learner(model_roberta, train_data=trn_roberta, val_data=val_roberta, batch_size =BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_roberta.fit_onecycle(2e-5, 4) #use hardware acceleartor while running this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = 'microsoft/deberta-base' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_3 = text.Transformer(model_3, maxlen=500, class_names=[0, 1,2,3,4,5,6,7,8,9,10])\n",
    "trn_3 = t_3.preprocess_train(x_train, y_train)\n",
    "val_3 = t_3.preprocess_test(x_test, y_test)\n",
    "\n",
    "model_deberta = t_3.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deberta.compile(loss=focal_loss(alpha=1, from_logits=True),optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_deberta = ktrain.get_learner(model_deberta, train_data=trn_roberta, val_data=val_roberta, batch_size =BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_deberta.fit_onecycle(2e-5, 4) #use hardware acceleartor while running this cell"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
